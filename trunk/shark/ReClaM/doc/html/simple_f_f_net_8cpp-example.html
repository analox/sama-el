<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
<title>ReClaM: simpleFFNet.cpp</title>
<link href="ReClaM.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.4.7 -->
<div class="tabs">
  <ul>
    <li><a href="index.html"><span>Main&nbsp;Page</span></a></li>
    <li><a href="namespaces.html"><span>Namespaces</span></a></li>
    <li><a href="annotated.html"><span>Classes</span></a></li>
    <li><a href="files.html"><span>Files</span></a></li>
    <li><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
    <li><a href="examples.html"><span>Examples</span></a></li>
  </ul></div>
<h1>simpleFFNet.cpp</h1>Offers the functions to create and to work with a feed-forward network. Together with an error measure and an optimization algorithm class you can use this feed-forward network class to produce your own optimization tool.<p>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
Please follow the link to view the source code of this example, that shows you, how you can construct your own feed forward neural network, completely with one error measure for training and another for monitoring and an optimization algorithm. The example itself can be executed in the example directory of package ReClaM.<p>
<dl compact><dt><b>Changes:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Status:</b></dt><dd>stable</dd></dl>
<div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="comment">//===========================================================================</span>
<a name="l00051"></a>00051 <span class="comment"></span><span class="comment">//===========================================================================</span>
<a name="l00052"></a>00052 
<a name="l00053"></a>00053 
<a name="l00054"></a>00054 <span class="preprocessor">#include "Array/Array.h"</span>
<a name="l00055"></a>00055 <span class="preprocessor">#include "<a class="code" href="_f_f_net_8h.html">ReClaM/FFNet.h</a>"</span>
<a name="l00056"></a>00056 <span class="preprocessor">#include "<a class="code" href="_rprop_8h.html">ReClaM/Rprop.h</a>"</span>
<a name="l00057"></a>00057 <span class="preprocessor">#include "<a class="code" href="_mean_squared_error_8h.html">ReClaM/MeanSquaredError.h</a>"</span>
<a name="l00058"></a>00058 <span class="preprocessor">#include "<a class="code" href="_error_measures_8h.html">ReClaM/ErrorMeasures.h</a>"</span>
<a name="l00059"></a>00059 <span class="preprocessor">#include "Rng/GlobalRng.h"</span>
<a name="l00060"></a>00060 
<a name="l00061"></a>00061 <span class="keyword">using namespace </span>std;
<a name="l00062"></a>00062 
<a name="l00063"></a>00063 
<a name="l00064"></a>00064 <span class="comment">// Define own network class to combine a feed forward</span>
<a name="l00065"></a>00065 <span class="comment">// neural network with a mean squared error measure for training</span>
<a name="l00066"></a>00066 <span class="comment">// and the RProp algorithm for optimization by resilent backpropagation.</span>
<a name="l00067"></a>00067 <span class="comment">// The mean squared error is only used for training, but the</span>
<a name="l00068"></a>00068 <span class="comment">// error percentage will be used to evaluate the fitness of the network.</span>
<a name="l00069"></a>00069 <span class="comment">// Then the best network depending on the error percentage will</span>
<a name="l00070"></a>00070 <span class="comment">// be used as final result:</span>
<a name="l00071"></a>00071 <span class="comment">//</span>
<a name="l00072"></a>00072 <span class="keyword">class </span>MyNet :   <span class="keyword">public</span> <a name="_a0"></a><a class="code" href="class_f_f_net.html">FFNet</a>,
<a name="l00073"></a>00073                 <span class="keyword">public</span> <a name="_a1"></a><a class="code" href="class_i_rprop_plus.html">IRpropPlus</a>,
<a name="l00074"></a>00074                 <span class="keyword">public</span> <a name="_a2"></a><a class="code" href="class_mean_squared_error.html">MeanSquaredError</a>,
<a name="l00075"></a>00075                 <span class="keyword">public</span> <a name="_a3"></a><a class="code" href="class_error_measures.html">ErrorMeasures</a>
<a name="l00076"></a>00076 {       
<a name="l00077"></a>00077 
<a name="l00078"></a>00078     <span class="keyword">public</span>:
<a name="l00079"></a>00079   
<a name="l00080"></a>00080         <span class="comment">// Constructor for a network based on a given connection</span>
<a name="l00081"></a>00081         <span class="comment">// matrix:</span>
<a name="l00082"></a>00082         MyNet( <span class="keywordtype">unsigned</span> nIn, <span class="keywordtype">unsigned</span> nOut, <span class="keyword">const</span> Array&lt; int &gt;&amp; cmat ) : 
<a name="l00083"></a>00083             <a name="a4"></a><a class="code" href="class_f_f_net.html#1b91d55ae87e513bc05ca9e6fc66ec1b">FFNet</a>( nIn, nOut, cmat ) { }
<a name="l00084"></a>00084 
<a name="l00085"></a>00085         <span class="comment">// Constructor for reading the network structure from</span>
<a name="l00086"></a>00086         <span class="comment">// a file:</span>
<a name="l00087"></a>00087         MyNet( <span class="keyword">const</span> string &amp;filename ) : <a class="code" href="class_f_f_net.html#1b91d55ae87e513bc05ca9e6fc66ec1b">FFNet</a>( filename ) { }
<a name="l00088"></a>00088   
<a name="l00089"></a>00089 
<a name="l00090"></a>00090         <span class="comment">// Define alternative sigmoid activation function for </span>
<a name="l00091"></a>00091         <span class="comment">// the hidden units:</span>
<a name="l00092"></a>00092         <span class="keywordtype">double</span> <a name="a5"></a><a class="code" href="class_f_f_net.html#ae1af26125b71f4145e8d6acff01d0b6">g</a>( <span class="keywordtype">double</span> a ) 
<a name="l00093"></a>00093         { 
<a name="l00094"></a>00094             <span class="keywordflow">return</span> a / ( 1 + fabs( a ) ); 
<a name="l00095"></a>00095         }
<a name="l00096"></a>00096   
<a name="l00097"></a>00097         <span class="comment">// A way to calculate the derivative of the</span>
<a name="l00098"></a>00098         <span class="comment">// activation function, when given the result</span>
<a name="l00099"></a>00099         <span class="comment">// of the activation:</span>
<a name="l00100"></a>00100         <span class="keywordtype">double</span> <a name="a6"></a><a class="code" href="class_f_f_net.html#87d90f96079c03024fff47938fe8cdb2">dg</a>( <span class="keywordtype">double</span> ga ) 
<a name="l00101"></a>00101         { 
<a name="l00102"></a>00102             <span class="keywordflow">return</span>  ( 1 - <a name="a7"></a><a class="code" href="class_i_rprop_plus.html#c2f5e90f44dcde38d7741fc181c0c329">sgn</a>( ga ) * ga ) * ( 1 - <a class="code" href="class_i_rprop_plus.html#c2f5e90f44dcde38d7741fc181c0c329">sgn</a>( ga ) * ga );
<a name="l00103"></a>00103         }
<a name="l00104"></a>00104   
<a name="l00105"></a>00105   
<a name="l00106"></a>00106         <span class="comment">// Use linear output neurons:</span>
<a name="l00107"></a>00107         <span class="keywordtype">double</span> <a name="a8"></a><a class="code" href="class_f_f_net.html#2fe2aa47a4c016ec9ab9b8054da4b023">gOutput</a>( <span class="keywordtype">double</span> a ) 
<a name="l00108"></a>00108         { 
<a name="l00109"></a>00109             <span class="keywordflow">return</span> a; 
<a name="l00110"></a>00110         }
<a name="l00111"></a>00111   
<a name="l00112"></a>00112         <span class="comment">// Because of linearity, the derivative</span>
<a name="l00113"></a>00113         <span class="comment">// the output neurons will always be a constant:</span>
<a name="l00114"></a>00114         <span class="keywordtype">double</span> <a name="a9"></a><a class="code" href="class_f_f_net.html#6c7c6bd2d1f4cafd418d004ec21a493a">dgOutput</a>(<span class="keywordtype">double</span> ga) 
<a name="l00115"></a>00115         { 
<a name="l00116"></a>00116             <span class="keywordflow">return</span> 1;
<a name="l00117"></a>00117         }
<a name="l00118"></a>00118 
<a name="l00119"></a>00119 };
<a name="l00120"></a>00120 
<a name="l00121"></a>00121 
<a name="l00122"></a>00122 <span class="keywordtype">int</span> main()
<a name="l00123"></a>00123 {
<a name="l00124"></a>00124     <span class="comment">// Just counter variables:</span>
<a name="l00125"></a>00125     <span class="keywordtype">unsigned</span> i,j;
<a name="l00126"></a>00126 
<a name="l00127"></a>00127 
<a name="l00128"></a>00128     <span class="keywordtype">unsigned</span> numberOfHiddenNeurons = 5;
<a name="l00129"></a>00129 
<a name="l00130"></a>00130     <span class="comment">// We use a network here with two input neurons, one</span>
<a name="l00131"></a>00131     <span class="comment">// output neuron and five hidden neurons:</span>
<a name="l00132"></a>00132     <span class="keywordtype">unsigned</span> numberOfNeurons = numberOfHiddenNeurons+2+1;
<a name="l00133"></a>00133 
<a name="l00134"></a>00134     <span class="keywordtype">unsigned</span> numberOfLearningCycles = 100;
<a name="l00135"></a>00135 
<a name="l00136"></a>00136 
<a name="l00137"></a>00137     <span class="comment">// Create connection matrix for the neurons:</span>
<a name="l00138"></a>00138     Array&lt; int &gt; con( numberOfNeurons, numberOfNeurons+1 );
<a name="l00139"></a>00139 
<a name="l00140"></a>00140     <span class="comment">// Establish connections from the input neurons to each </span>
<a name="l00141"></a>00141     <span class="comment">// hidden neuron and from each hidden neuron to the output neuron:</span>
<a name="l00142"></a>00142     <span class="keywordflow">for</span> ( i = 2; i &lt; con.dim( 0 ); i++ )
<a name="l00143"></a>00143     {
<a name="l00144"></a>00144         <span class="keywordflow">for</span> ( j = 0; j &lt; i; j++ )
<a name="l00145"></a>00145         {
<a name="l00146"></a>00146             con( i, j ) = 1;
<a name="l00147"></a>00147         }
<a name="l00148"></a>00148     }
<a name="l00149"></a>00149 
<a name="l00150"></a>00150     <span class="comment">// Set bias values:</span>
<a name="l00151"></a>00151     <span class="keywordflow">for</span> ( i = 2; i &lt; con.dim( 0 ); i++ )
<a name="l00152"></a>00152     {
<a name="l00153"></a>00153         con( i, con.dim( 1 )-1 ) = 1;
<a name="l00154"></a>00154     }
<a name="l00155"></a>00155   
<a name="l00156"></a>00156     <span class="comment">// Construct two new network objects on the base of the</span>
<a name="l00157"></a>00157     <span class="comment">// connection matrix. The first one will be used for</span>
<a name="l00158"></a>00158     <span class="comment">// training, the second one will store the network</span>
<a name="l00159"></a>00159     <span class="comment">// with the smallest validation error:</span>
<a name="l00160"></a>00160     MyNet net( 2, 1, con ), 
<a name="l00161"></a>00161           netMin( 2, 1, con );
<a name="l00162"></a>00162 
<a name="l00163"></a>00163 
<a name="l00164"></a>00164     <span class="comment">// Initialize the weights of the neuron connections</span>
<a name="l00165"></a>00165     <span class="comment">// by uniformally distributed random numbers between</span>
<a name="l00166"></a>00166     <span class="comment">// "-0.1" and "0.1":</span>
<a name="l00167"></a>00167     net.initWeights( -0.1, 0.1 );
<a name="l00168"></a>00168 
<a name="l00169"></a>00169     <span class="comment">// The untrained network is the best one until now.</span>
<a name="l00170"></a>00170     netMin = net;
<a name="l00171"></a>00171   
<a name="l00172"></a>00172     <span class="comment">// Use two distinct data sets. One will only be used for</span>
<a name="l00173"></a>00173     <span class="comment">// training, the other will be used to examine</span>
<a name="l00174"></a>00174     <span class="comment">// the fitness of the trained network:</span>
<a name="l00175"></a>00175     Array&lt; double &gt; inputTrain( 121, 2<a name="a10"></a><a class="code" href="_adp_b_p_8h.html#98d9ad7e268c9990df6f886e49933903">u</a> ), targetTrain( 121, 1<a class="code" href="_adp_b_p_8h.html#98d9ad7e268c9990df6f886e49933903">u</a> );
<a name="l00176"></a>00176     Array&lt; double &gt; inputValidate( 121, 2<a class="code" href="_adp_b_p_8h.html#98d9ad7e268c9990df6f886e49933903">u</a> ), targetValidate( 121,1<a class="code" href="_adp_b_p_8h.html#98d9ad7e268c9990df6f886e49933903">u</a> );
<a name="l00177"></a>00177 
<a name="l00178"></a>00178     <span class="comment">// Construct data:</span>
<a name="l00179"></a>00179     <span class="keywordflow">for</span> ( i = 0; i &lt; 6; i++ )
<a name="l00180"></a>00180     {
<a name="l00181"></a>00181         <span class="keywordflow">for</span> ( j = 0; j &lt; 6; j++ )
<a name="l00182"></a>00182         {
<a name="l00183"></a>00183             inputTrain( i * 6 + j, 0 ) = i * 0.4 - 1;
<a name="l00184"></a>00184             inputTrain( i * 6 + j, 1 ) = j * 0.4 - 1;
<a name="l00185"></a>00185 
<a name="l00186"></a>00186             inputValidate( i * 6 + j, 0 ) = i * 0.4 - 1;
<a name="l00187"></a>00187             inputValidate( i * 6 + j, 1 ) = j * 0.4 - 1;
<a name="l00188"></a>00188 
<a name="l00189"></a>00189             targetTrain( i * 6 + j, 0 ) = 
<a name="l00190"></a>00190                 ( i * 0.4 - 1 ) * ( j * 0.4 - 1 ) + Rng::gauss( 0, 0.2 );
<a name="l00191"></a>00191 
<a name="l00192"></a>00192             targetValidate( i * 6 + j, 0 ) = 
<a name="l00193"></a>00193                 ( i * 0.4 - 1 ) * ( j * 0.4 - 1 ) + Rng::gauss( 0, 0.2 );
<a name="l00194"></a>00194         }
<a name="l00195"></a>00195     }
<a name="l00196"></a>00196 
<a name="l00197"></a>00197 
<a name="l00198"></a>00198     <span class="comment">// Current error percentage of the training set:</span>
<a name="l00199"></a>00199     <span class="keywordtype">double</span> t; 
<a name="l00200"></a>00200       
<a name="l00201"></a>00201     <span class="comment">// Current error percentage of the validation set:</span>
<a name="l00202"></a>00202     <span class="keywordtype">double</span> <a name="a11"></a><a class="code" href="_adp_b_p_8h.html#e9ef92f25cf31ee79e03f1a9d1108609">v</a>;
<a name="l00203"></a>00203 
<a name="l00204"></a>00204     <span class="comment">// The smallest error percentage for the validation set:</span>
<a name="l00205"></a>00205     <span class="keywordtype">double</span> vMin = net.errorPercentage( inputValidate, targetValidate );
<a name="l00206"></a>00206 
<a name="l00207"></a>00207     <span class="comment">// The training epoch, where the network with the</span>
<a name="l00208"></a>00208     <span class="comment">// smallest error percentage for the validation set </span>
<a name="l00209"></a>00209     <span class="comment">// occured:</span>
<a name="l00210"></a>00210     <span class="keywordtype">unsigned</span> epochMin = 0;
<a name="l00211"></a>00211 
<a name="l00212"></a>00212     <span class="comment">// Initialize the RProp optimization algorithm:</span>
<a name="l00213"></a>00213     net.initRprop( 0.01 );
<a name="l00214"></a>00214 
<a name="l00215"></a>00215 
<a name="l00216"></a>00216     cout &lt;&lt; <span class="stringliteral">"Train network:\n"</span> &lt;&lt; endl 
<a name="l00217"></a>00217          &lt;&lt; <span class="stringliteral">"epoch:\ttraining error:\tvalidation error:"</span> &lt;&lt; endl
<a name="l00218"></a>00218          &lt;&lt; <span class="stringliteral">"-----------------------------------------"</span> &lt;&lt; endl;
<a name="l00219"></a>00219 
<a name="l00220"></a>00220     <span class="comment">// Training of the network:</span>
<a name="l00221"></a>00221     <span class="keywordflow">for</span> ( <span class="keywordtype">unsigned</span> epoch = 1; epoch &lt;= numberOfLearningCycles; epoch++ ) 
<a name="l00222"></a>00222     {
<a name="l00223"></a>00223         <span class="comment">// Train the net with RProp ...</span>
<a name="l00224"></a>00224         net.rprop( inputTrain, targetTrain, 1.2, .5, 50, 0 );
<a name="l00225"></a>00225 
<a name="l00226"></a>00226         <span class="comment">//  ... and calculate the (monitoring) errors:</span>
<a name="l00227"></a>00227         t = net.errorPercentage( inputTrain, targetTrain );
<a name="l00228"></a>00228         v = net.errorPercentage( inputValidate, targetValidate );
<a name="l00229"></a>00229 
<a name="l00230"></a>00230         <span class="comment">// Monitor the results:</span>
<a name="l00231"></a>00231         std::cout &lt;&lt; epoch &lt;&lt; <span class="stringliteral">"\t"</span> &lt;&lt; t &lt;&lt; <span class="stringliteral">"\t\t"</span> &lt;&lt; v &lt;&lt; endl;
<a name="l00232"></a>00232 
<a name="l00233"></a>00233         <span class="comment">// Memorize the network with smallest validation error:</span>
<a name="l00234"></a>00234         <span class="keywordflow">if</span> ( v &lt; vMin ) 
<a name="l00235"></a>00235         {
<a name="l00236"></a>00236             vMin = v;
<a name="l00237"></a>00237             netMin = net;
<a name="l00238"></a>00238             epochMin = epoch;
<a name="l00239"></a>00239         }
<a name="l00240"></a>00240     }
<a name="l00241"></a>00241     
<a name="l00242"></a>00242     <span class="comment">//  Output of the performance values of the best network:</span>
<a name="l00243"></a>00243     t = netMin.errorPercentage( inputTrain, targetTrain );
<a name="l00244"></a>00244     v = netMin.errorPercentage( inputValidate, targetValidate );
<a name="l00245"></a>00245 
<a name="l00246"></a>00246     cout &lt;&lt; endl &lt;&lt; <span class="stringliteral">"\n\nError of network with best validation error:\n"</span> 
<a name="l00247"></a>00247          &lt;&lt; endl
<a name="l00248"></a>00248          &lt;&lt; <span class="stringliteral">"epoch:\ttraining error:\tvalidation error:"</span> &lt;&lt; endl
<a name="l00249"></a>00249          &lt;&lt; <span class="stringliteral">"-----------------------------------------"</span> &lt;&lt; endl;
<a name="l00250"></a>00250     cout &lt;&lt; epochMin &lt;&lt; <span class="stringliteral">"\t"</span> &lt;&lt; t &lt;&lt; <span class="stringliteral">"\t"</span> &lt;&lt; v &lt;&lt; endl;
<a name="l00251"></a>00251     
<a name="l00252"></a>00252     <span class="comment">//  Output of the structure of the best network:</span>
<a name="l00253"></a>00253     cout &lt;&lt; <span class="stringliteral">"\n\n\nStructure of this network\n(no. of input neurons, "</span>
<a name="l00254"></a>00254          &lt;&lt; <span class="stringliteral">"no. of output neurons, connection matrix, weight matrix):\n"</span> 
<a name="l00255"></a>00255          &lt;&lt; endl;
<a name="l00256"></a>00256     cout.precision( 6 );
<a name="l00257"></a>00257     cout.setf( ios::fixed | ios::showpos );
<a name="l00258"></a>00258     cout &lt;&lt; netMin &lt;&lt; endl;
<a name="l00259"></a>00259     
<a name="l00260"></a>00260     <span class="comment">// Show network behaviour for one input pattern:</span>
<a name="l00261"></a>00261     cout &lt;&lt; <span class="stringliteral">"Processing a single input pattern:\n"</span> &lt;&lt; endl;
<a name="l00262"></a>00262     Array&lt; double &gt; <a name="a12"></a><a class="code" href="_adp_b_p_8h.html#faabd99e32d62d06313dac12ae518da6">in</a>( 2 ), <a name="a13"></a><a class="code" href="_adp_b_p_8h.html#f8434108d670ecbc17d19b1540344c25">out</a>( 1 );
<a name="l00263"></a>00263     <a class="code" href="_adp_b_p_8h.html#faabd99e32d62d06313dac12ae518da6">in</a>( 0 ) = 0.3;
<a name="l00264"></a>00264     <a class="code" href="_adp_b_p_8h.html#faabd99e32d62d06313dac12ae518da6">in</a>( 1 ) = -0.1;
<a name="l00265"></a>00265     netMin.<a name="a14"></a><a class="code" href="class_model_interface.html#e7c9c3ab5397b01628d254b71f941784">model</a>( <a class="code" href="_adp_b_p_8h.html#faabd99e32d62d06313dac12ae518da6">in</a>, <a class="code" href="_adp_b_p_8h.html#f8434108d670ecbc17d19b1540344c25">out</a> );
<a name="l00266"></a>00266     cout &lt;&lt; <span class="stringliteral">"Input:\t( "</span> &lt;&lt; <a class="code" href="_adp_b_p_8h.html#faabd99e32d62d06313dac12ae518da6">in</a>( 0 ) &lt;&lt; <span class="stringliteral">", "</span> &lt;&lt; <a class="code" href="_adp_b_p_8h.html#faabd99e32d62d06313dac12ae518da6">in</a>( 1 ) &lt;&lt; <span class="stringliteral">" ) "</span> &lt;&lt; endl;
<a name="l00267"></a>00267     cout &lt;&lt; <span class="stringliteral">"Output:\t"</span> &lt;&lt; <a class="code" href="_adp_b_p_8h.html#f8434108d670ecbc17d19b1540344c25">out</a>( 0 ) &lt;&lt; endl;
<a name="l00268"></a>00268     
<a name="l00269"></a>00269     exit( EXIT_SUCCESS );
<a name="l00270"></a>00270 }
<a name="l00271"></a>00271 
<a name="l00272"></a>00272 
<a name="l00273"></a>00273 
<a name="l00274"></a>00274 
<a name="l00275"></a>00275 
</pre></div> <hr size="1"><address style="align: right;"><small>Generated on Wed Dec 16 13:07:16 2009 for ReClaM by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.4.7 </small></address>
</body>
</html>
