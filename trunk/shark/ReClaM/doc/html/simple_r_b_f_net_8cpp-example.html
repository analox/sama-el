<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
<title>ReClaM: simpleRBFNet.cpp</title>
<link href="ReClaM.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.4.7 -->
<div class="tabs">
  <ul>
    <li><a href="index.html"><span>Main&nbsp;Page</span></a></li>
    <li><a href="namespaces.html"><span>Namespaces</span></a></li>
    <li><a href="annotated.html"><span>Classes</span></a></li>
    <li><a href="files.html"><span>Files</span></a></li>
    <li><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
    <li><a href="examples.html"><span>Examples</span></a></li>
  </ul></div>
<h1>simpleRBFNet.cpp</h1>Offers the functions to create and to work with radial basis function networks and to train it with the mean squared error. This combination provides more computational efficiency compared to using the class <a class="el" href="class_mean_squared_error.html">MeanSquaredError</a> Together with an error measure and an optimization algorithm class you can use this radial basis function network class to produce your own optimization tool. Radial Basis Function networks are especially used for classification tasks. For this purpose the class of models contains, besides the number of hidden neurons <img class="formulaInl" alt="$nHidden$" src="form_228.png">, 4 different kinds of parameters: the centers of the hidden neurons <img class="formulaInl" alt="$\vec{m}_j=(m_{j1},\ldots,m_{jn})$" src="form_229.png">, the standard deviations of the hidden neurons <img class="formulaInl" alt="$\vec{\sigma}_j=(\sigma_{j1},\ldots,\sigma_{jn})$" src="form_230.png">, the weights of the connections between the hidden neuron with index <img class="formulaInl" alt="$j$" src="form_25.png"> and output neuron with index <img class="formulaInl" alt="$k$" src="form_231.png"> and the bias <img class="formulaInl" alt="$b_k$" src="form_232.png"> of the output neuron with index <img class="formulaInl" alt="$k$" src="form_231.png">. The output neuron with index <img class="formulaInl" alt="$k$" src="form_231.png"> yields the following output, assuming a <img class="formulaInl" alt="$nInput$" src="form_233.png">-dimensional input vector <img class="formulaInl" alt="$\vec{x}$" src="form_234.png">:<p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ y_k = b_k + \sum_{j=1}^{nHidden} \exp\left[ -\frac{1}{2}\sum_{i=1}^{nInput} \frac{\left(x_i-m_{ji}\right)2}{\sigma_{ji}}\right] \]" src="form_235.png">
<p>
<p>
The mean squared error is used as the error measure.<p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ E = \frac{1}{P\cdot nOutput}\sum_{p=1}^{P}\sum_{k=1}^{nOutput} \left(y_k^{(p)}-t_k^{(p)}\right)2 \]" src="form_236.png">
<p>
<p>
Here, <img class="formulaInl" alt="$P$" src="form_41.png"> denotes the number of different patterns, <img class="formulaInl" alt="$y_k^{(p)}$" src="form_237.png"> and <img class="formulaInl" alt="$t_k^{(p)}$" src="form_238.png"> the output and target value belonging to the <img class="formulaInl" alt="$p^{th}$" src="form_239.png"> input pattern and <img class="formulaInl" alt="$nOutput$" src="form_240.png"> the number of output neurons.<p>
Please follow the link to view the source code of this example, that shows you, how you can construct your own radial basis function neural network, completely with one error measure for training and another for monitoring and an optimization algorithm. The example itself can be executed in the example directory of package ReClaM.<p>
<dl compact><dt><b>Author:</b></dt><dd>M. H&uuml;sken </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>2001</dd></dl>
<dl compact><dt><b>Changes:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Status:</b></dt><dd>stable</dd></dl>
<div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="comment">//===========================================================================</span>
<a name="l00051"></a>00051 <span class="comment"></span><span class="comment">//===========================================================================</span>
<a name="l00052"></a>00052 
<a name="l00053"></a>00053 
<a name="l00054"></a>00054 <span class="preprocessor">#include &lt;<a class="code" href="_rprop_8h.html">ReClaM/Rprop.h</a>&gt;</span>
<a name="l00055"></a>00055 <span class="preprocessor">#include &lt;<a class="code" href="_r_b_f_net_8h.html">ReClaM/RBFNet.h</a>&gt;</span>
<a name="l00056"></a>00056 <span class="preprocessor">#include &lt;<a class="code" href="_error_measures_8h.html">ReClaM/ErrorMeasures.h</a>&gt;</span>
<a name="l00057"></a>00057 <span class="preprocessor">#include &lt;<a class="code" href="_mean_squared_error_8h.html">ReClaM/MeanSquaredError.h</a>&gt;</span>
<a name="l00058"></a>00058 <span class="preprocessor">#include &lt;Array/ArrayOp.h&gt;</span>
<a name="l00059"></a>00059 <span class="preprocessor">#include &lt;Rng/GlobalRng.h&gt;</span>
<a name="l00060"></a>00060 
<a name="l00061"></a>00061 <span class="keyword">using namespace </span>std;
<a name="l00062"></a>00062 
<a name="l00063"></a>00063 
<a name="l00064"></a>00064 <span class="comment">// Define own network class to combine a radial basis function</span>
<a name="l00065"></a>00065 <span class="comment">// neural network with a mean squared error measure for training</span>
<a name="l00066"></a>00066 <span class="comment">// and the RProp algorithm for optimization:</span>
<a name="l00067"></a>00067 <span class="comment">// For monitoring purposes, there is also included the</span>
<a name="l00068"></a>00068 <span class="comment">// binary criterion error measure.</span>
<a name="l00069"></a>00069 <span class="comment">//</span>
<a name="l00070"></a>00070 <span class="keyword">class </span>MyRBFN : <span class="keyword">public</span> <a name="_a15"></a><a class="code" href="class_r_b_f_net.html">RBFNet</a>,
<a name="l00071"></a>00071                <span class="keyword">public</span> <a name="_a16"></a><a class="code" href="class_i_rprop_plus.html">IRpropPlus</a>,
<a name="l00072"></a>00072                <span class="keyword">public</span> <a name="_a17"></a><a class="code" href="class_mean_squared_error.html">MeanSquaredError</a>,
<a name="l00073"></a>00073                <span class="keyword">public</span> <a name="_a18"></a><a class="code" href="class_error_measures.html">ErrorMeasures</a>
<a name="l00074"></a>00074 {
<a name="l00075"></a>00075     <span class="keyword">public</span>:
<a name="l00076"></a>00076 
<a name="l00077"></a>00077         <span class="comment">// Constructor for a network with basis structure but no content:</span>
<a name="l00078"></a>00078         MyRBFN( <span class="keywordtype">unsigned</span> numInput, <span class="keywordtype">unsigned</span> numOutput, <span class="keywordtype">unsigned</span> numHidden )
<a name="l00079"></a>00079             : <a name="a19"></a><a class="code" href="class_r_b_f_net.html#4cbb87888df3bac2a2b9426b4ced0473">RBFNet</a>( numInput, numOutput, numHidden ) { }
<a name="l00080"></a>00080 
<a name="l00081"></a>00081 };
<a name="l00082"></a>00082 
<a name="l00083"></a>00083 
<a name="l00084"></a>00084 
<a name="l00085"></a>00085 <span class="keywordtype">int</span> main( )
<a name="l00086"></a>00086 { 
<a name="l00087"></a>00087     <span class="comment">// Just counter variables:</span>
<a name="l00088"></a>00088     <span class="keywordtype">unsigned</span> i, t;
<a name="l00089"></a>00089 
<a name="l00090"></a>00090     <span class="comment">// No. of neurons (input, output, hidden):</span>
<a name="l00091"></a>00091     <span class="keyword">const</span> <span class="keywordtype">unsigned</span> inputDimension = 1;
<a name="l00092"></a>00092     <span class="keyword">const</span> <span class="keywordtype">unsigned</span> outputDimension = 1; 
<a name="l00093"></a>00093     <span class="keyword">const</span> <span class="keywordtype">unsigned</span> nHidden = 2;
<a name="l00094"></a>00094 
<a name="l00095"></a>00095     <span class="comment">// Use 100 input patterns:</span>
<a name="l00096"></a>00096     <span class="keyword">const</span> <span class="keywordtype">unsigned</span> numSamples = 100;
<a name="l00097"></a>00097   
<a name="l00098"></a>00098     <span class="comment">// Define input and target patterns:</span>
<a name="l00099"></a>00099     Array&lt; double &gt; inTrain ( numSamples, inputDimension ),
<a name="l00100"></a>00100                     outTrain( numSamples, outputDimension );
<a name="l00101"></a>00101 
<a name="l00102"></a>00102 
<a name="l00103"></a>00103     <span class="comment">// Create data set using gauss distributed random numbers:</span>
<a name="l00104"></a>00104     <span class="keywordflow">for</span> ( i = 0; i &lt; numSamples / 2; i++ )
<a name="l00105"></a>00105     {
<a name="l00106"></a>00106         inTrain( i, 0 ) = Rng::gauss( 2.0, 0.3 );
<a name="l00107"></a>00107         outTrain( i, 0 ) = 0;
<a name="l00108"></a>00108         inTrain( i + numSamples / 2, 0 ) = Rng::gauss( 5.0, 0.5 );
<a name="l00109"></a>00109         outTrain( i + numSamples / 2, 0 ) = 1;
<a name="l00110"></a>00110     }
<a name="l00111"></a>00111   
<a name="l00112"></a>00112     <span class="comment">// Create an empty network with two input and hidden neurons and</span>
<a name="l00113"></a>00113     <span class="comment">// one output neuron:</span>
<a name="l00114"></a>00114     MyRBFN rbfn( inputDimension, outputDimension, nHidden );     
<a name="l00115"></a>00115 
<a name="l00116"></a>00116     <span class="comment">// Use the input and target patterns for initialization of</span>
<a name="l00117"></a>00117     <span class="comment">// the network:</span>
<a name="l00118"></a>00118     rbfn.initRBFNet( inTrain, outTrain );
<a name="l00119"></a>00119 
<a name="l00120"></a>00120     <span class="comment">// Show performance of the network after initialization:</span>
<a name="l00121"></a>00121     cout &lt;&lt; <span class="stringliteral">"After initialization of RBFN:\n"</span> &lt;&lt; endl;
<a name="l00122"></a>00122     cout &lt;&lt; <span class="stringliteral">"Mean squared error:\t"</span> 
<a name="l00123"></a>00123          &lt;&lt; rbfn.meanSquaredError( inTrain, outTrain ) &lt;&lt; endl;
<a name="l00124"></a>00124     cout &lt;&lt; <span class="stringliteral">"Classification error:\t"</span> 
<a name="l00125"></a>00125          &lt;&lt; rbfn.binaryCriterion( inTrain, outTrain ) &lt;&lt; endl;
<a name="l00126"></a>00126 
<a name="l00127"></a>00127     <span class="comment">// Extract center, variance and weights of connection to</span>
<a name="l00128"></a>00128     <span class="comment">// output neurons for the hidden neurons...</span>
<a name="l00129"></a>00129     Array&lt; double &gt; c = rbfn.getCenter( );
<a name="l00130"></a>00130     Array&lt; double &gt; v = rbfn.getVariance( );
<a name="l00131"></a>00131     Array&lt; double &gt; w = rbfn.getWeights( );
<a name="l00132"></a>00132 
<a name="l00133"></a>00133     <span class="comment">// ...and monitor the data:</span>
<a name="l00134"></a>00134     cout &lt;&lt; <span class="stringliteral">"Data of the "</span> &lt;&lt; c.dim( 0 ) &lt;&lt; <span class="stringliteral">" hidden neurons:"</span> &lt;&lt; endl;
<a name="l00135"></a>00135     <span class="keywordflow">for</span> ( i = 0; i &lt; c.dim( 0 ); i++ )
<a name="l00136"></a>00136     {
<a name="l00137"></a>00137         cout &lt;&lt; i + 1 &lt;&lt; <span class="stringliteral">". neuron:\n\tcenter = "</span> &lt;&lt; c( i, 0 ) 
<a name="l00138"></a>00138              &lt;&lt; <span class="stringliteral">",\n\tvariance = "</span> 
<a name="l00139"></a>00139              &lt;&lt; <a name="a20"></a><a class="code" href="_adp_b_p_8h.html#e9ef92f25cf31ee79e03f1a9d1108609">v</a>( i, 0 ) &lt;&lt; <span class="stringliteral">",\n\tweight to output neuron = "</span> &lt;&lt; w( 0 , i ) 
<a name="l00140"></a>00140              &lt;&lt; <span class="stringliteral">"."</span> &lt;&lt; endl;
<a name="l00141"></a>00141     }
<a name="l00142"></a>00142 
<a name="l00143"></a>00143     cout &lt;&lt; <span class="stringliteral">"\n\nTraining of the network:\n"</span> &lt;&lt; endl;
<a name="l00144"></a>00144 
<a name="l00145"></a>00145     <span class="comment">// Initialize the RProp optimization algorithm:</span>
<a name="l00146"></a>00146     rbfn.initRprop( 0.0125 );
<a name="l00147"></a>00147 
<a name="l00148"></a>00148     cout &lt;&lt; <span class="stringliteral">"epoch:\ttraining error:\tclassification error:"</span> &lt;&lt; endl;
<a name="l00149"></a>00149     cout &lt;&lt; <span class="stringliteral">"---------------------------------------------"</span> &lt;&lt; endl;
<a name="l00150"></a>00150     <span class="comment">// Train the network for ten epochs:</span>
<a name="l00151"></a>00151     <span class="keywordflow">for</span> ( t = 0; t &lt; 10; ++t ) 
<a name="l00152"></a>00152     {
<a name="l00153"></a>00153         <span class="comment">// Use RProp for training with all patterns:</span>
<a name="l00154"></a>00154         rbfn.rprop( inTrain, outTrain );
<a name="l00155"></a>00155 
<a name="l00156"></a>00156         <span class="comment">// Output of training and monitoring error:</span>
<a name="l00157"></a>00157         cout &lt;&lt; t &lt;&lt; <span class="charliteral">'\t'</span>
<a name="l00158"></a>00158              &lt;&lt; rbfn.error( inTrain, outTrain ) &lt;&lt; <span class="stringliteral">"\t"</span>
<a name="l00159"></a>00159              &lt;&lt; rbfn.binaryCriterion( inTrain, outTrain ) &lt;&lt; <span class="stringliteral">"\t"</span>
<a name="l00160"></a>00160              &lt;&lt; endl;
<a name="l00161"></a>00161     }
<a name="l00162"></a>00162 
<a name="l00163"></a>00163     cout &lt;&lt; <span class="stringliteral">"\n\nAfter Training of RBFN:\n"</span> &lt;&lt; endl;
<a name="l00164"></a>00164 
<a name="l00165"></a>00165     <span class="comment">// Show performance of the network after training:</span>
<a name="l00166"></a>00166     cout &lt;&lt; <span class="stringliteral">"Mean squared error:\t"</span> 
<a name="l00167"></a>00167          &lt;&lt; rbfn.meanSquaredError( inTrain, outTrain ) &lt;&lt; endl;
<a name="l00168"></a>00168     cout &lt;&lt; <span class="stringliteral">"Classification error:\t"</span> 
<a name="l00169"></a>00169          &lt;&lt; rbfn.binaryCriterion( inTrain, outTrain ) &lt;&lt; endl;
<a name="l00170"></a>00170 
<a name="l00171"></a>00171     <span class="comment">// Extract center, variance and weights of connection to</span>
<a name="l00172"></a>00172     <span class="comment">// output neurons for the hidden neurons...</span>
<a name="l00173"></a>00173     c = rbfn.getCenter( );
<a name="l00174"></a>00174     v = rbfn.getVariance( );
<a name="l00175"></a>00175     w = rbfn.getWeights( );
<a name="l00176"></a>00176 
<a name="l00177"></a>00177     <span class="comment">// ...and monitor the data:</span>
<a name="l00178"></a>00178     cout &lt;&lt; <span class="stringliteral">"Data of the "</span> &lt;&lt; c.dim( 0 ) &lt;&lt; <span class="stringliteral">" hidden neurons:"</span> &lt;&lt; endl;
<a name="l00179"></a>00179     <span class="keywordflow">for</span> ( i = 0; i &lt; c.dim( 0 ); i++ )
<a name="l00180"></a>00180     {
<a name="l00181"></a>00181         cout &lt;&lt; i + 1 &lt;&lt; <span class="stringliteral">". neuron:\n\tcenter = "</span> &lt;&lt; c( i, 0 ) 
<a name="l00182"></a>00182              &lt;&lt; <span class="stringliteral">",\n\tvariance = "</span> &lt;&lt; <a class="code" href="_adp_b_p_8h.html#e9ef92f25cf31ee79e03f1a9d1108609">v</a>( i, 0 ) 
<a name="l00183"></a>00183              &lt;&lt; <span class="stringliteral">",\n\tweight to output neuron = "</span> 
<a name="l00184"></a>00184              &lt;&lt; w( 0, i ) &lt;&lt; <span class="stringliteral">"."</span> &lt;&lt; endl;
<a name="l00185"></a>00185     }
<a name="l00186"></a>00186 
<a name="l00187"></a>00187     exit( EXIT_SUCCESS );
<a name="l00188"></a>00188 }
<a name="l00189"></a>00189 
</pre></div> <hr size="1"><address style="align: right;"><small>Generated on Wed Dec 16 13:07:16 2009 for ReClaM by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.4.7 </small></address>
</body>
</html>
