<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
<title>ReClaM: AdpBP90b Class Reference</title>
<link href="ReClaM.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.4.7 -->
<div class="tabs">
  <ul>
    <li><a href="index.html"><span>Main&nbsp;Page</span></a></li>
    <li><a href="namespaces.html"><span>Namespaces</span></a></li>
    <li id="current"><a href="annotated.html"><span>Classes</span></a></li>
    <li><a href="files.html"><span>Files</span></a></li>
    <li><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
    <li><a href="examples.html"><span>Examples</span></a></li>
  </ul></div>
<div class="tabs">
  <ul>
    <li><a href="annotated.html"><span>Class&nbsp;List</span></a></li>
    <li><a href="hierarchy.html"><span>Class&nbsp;Hierarchy</span></a></li>
    <li><a href="functions.html"><span>Class&nbsp;Members</span></a></li>
  </ul></div>
<h1>AdpBP90b Class Reference</h1><!-- doxytag: class="AdpBP90b" --><!-- doxytag: inherits="ModelInterface" -->Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent.  
<a href="#_details">More...</a>
<p>
<code>#include &lt;<a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>&gt;</code>
<p>
<p>Inheritance diagram for AdpBP90b:
<p><center><img src="class_adp_b_p90b.png" usemap="#AdpBP90b_map" border="0" alt=""></center>
<map name="AdpBP90b_map">
<area href="class_model_interface.html" alt="ModelInterface" shape="rect" coords="0,0,97,24">
</map>
<a href="class_adp_b_p90b-members.html">List of all members.</a><table border="0" cellpadding="0" cellspacing="0">
<tr><td></td></tr>
<tr><td colspan="2"><br><h2>Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_adp_b_p90b.html#dd7fe317e8f0f253bbe6015fce4eac05">initAdpBP</a> (double _eta0=0.1)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Prepares the optimization algorithm for the currently used model.  <a href="#dd7fe317e8f0f253bbe6015fce4eac05"></a><br></td></tr>
</table>
<hr><a name="_details"></a><h2>Detailed Description</h2>
Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. 
<p>
<dl compact><dt><b></b></dt><dd>To avoid bad choices of the learning rate, the algorithm by Silva and Almeida uses an individual adaptive learning rate <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> for each weight <img class="formulaInl" alt="$w_i$" src="form_79.png">. The adaptation of <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> is determined by the sign of the partial derivation <img class="formulaInl" alt="$\frac{\partial E}{\partial w_i}$" src="form_80.png">. If the sign of the derivation changes from iteration <img class="formulaInl" alt="$t - 1$" src="form_81.png"> to iteration <img class="formulaInl" alt="$t$" src="form_82.png">, then at least one minimum with regard to <img class="formulaInl" alt="$w_i$" src="form_79.png"> was skipped and hence the learning rate <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> is decreased by the multiplication with a predefined constant. If there is no change of the sign, it a plateau of the objective function is assumed and the learning rate <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> is increased by the multiplication with a second constant. Furthermore, weight-backtracking is used, i.e.. if the error increases from iteration <img class="formulaInl" alt="$t - 1$" src="form_81.png"> to <img class="formulaInl" alt="$t$" src="form_82.png">, then the last weight adaptation is undone. For further information about this algorithm, please refer to:</dd></dl>
<div class="fragment"><pre class="fragment"> @InCollection{silva:90b,
  author =       {Fernando M. Silva and Luis B. Almeida},
  title =        {Acceleration Techniques for the Backpropagation Algorithm},
  booktitle =    {Neural Networks -- EURASIP Workshop 1990},
  pages =        {110-119},
  year =         {1990},
  editor =       {Luis B. Almeida and C. J. Wellekens},
  number =       {412},
  series =       {LNCS},
  publisher = {Springer-Verlag},
 }
 </pre></div> This second version of the algorithm has some minor modifications that improve the performance.<p>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
<dl compact><dt><b>Changes:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Status:</b></dt><dd>stable </dd></dl>

<p>

<p>
Definition at line <a class="el" href="_adp_b_p_8h-source.html#l00343">343</a> of file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.<hr><h2>Member Function Documentation</h2>
<a class="anchor" name="dd7fe317e8f0f253bbe6015fce4eac05"></a><!-- doxytag: member="AdpBP90b::initAdpBP" ref="dd7fe317e8f0f253bbe6015fce4eac05" args="(double _eta0=0.1)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void AdpBP90b::initAdpBP           </td>
          <td>(</td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>_eta0</em> = <code>0.1</code>          </td>
          <td>&nbsp;)&nbsp;</td>
          <td width="100%"><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Prepares the optimization algorithm for the currently used model. 
<p>
Internal variables of the class instance are initialized. An initial learning rate <img class="formulaInl" alt="$\eta_0$" src="form_87.png"> is assigned to all weights of the network.<p>
<dl compact><dt><b>Parameters:</b></dt><dd>
  <table border="0" cellspacing="2" cellpadding="0">
    <tr><td valign="top"></td><td valign="top"><em>_eta0</em>&nbsp;</td><td>Initial value for the adaptive learning rate <img class="formulaInl" alt="$\eta$" src="form_86.png"> for all values. The default value is "0.1". </td></tr>
  </table>
</dl>
<dl compact><dt><b>Returns:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
<dl compact><dt><b>Changes</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Status</b></dt><dd>stable </dd></dl>

<p>
Definition at line <a class="el" href="_adp_b_p_8h-source.html#l00370">370</a> of file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.
<p>
References <a class="el" href="_adp_b_p_8h-source.html#l00496">dedwOld</a>, <a class="el" href="_adp_b_p_8h-source.html#l00489">deltaw</a>, <a class="el" href="_adp_b_p_8h-source.html#l00500">eta</a>, <a class="el" href="_adp_b_p_8h-source.html#l00482">eta0</a>, <a class="el" href="_adp_b_p_8h-source.html#l00485">oldE</a>, <a class="el" href="_adp_b_p_8h-source.html#l00493">v</a>, and <a class="el" href="_model_interface_8h-source.html#l00199">ModelInterface::w</a>.
</div>
</div><p>
<hr>The documentation for this class was generated from the following file:<ul>
<li><a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a></ul>
<hr size="1"><address style="align: right;"><small>Generated on Wed Dec 16 13:07:16 2009 for ReClaM by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.4.7 </small></address>
</body>
</html>
