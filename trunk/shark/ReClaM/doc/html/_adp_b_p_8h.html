<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
<title>ReClaM: AdpBP.h File Reference</title>
<link href="ReClaM.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.4.7 -->
<div class="tabs">
  <ul>
    <li><a href="index.html"><span>Main&nbsp;Page</span></a></li>
    <li><a href="namespaces.html"><span>Namespaces</span></a></li>
    <li><a href="annotated.html"><span>Classes</span></a></li>
    <li id="current"><a href="files.html"><span>Files</span></a></li>
    <li><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
    <li><a href="examples.html"><span>Examples</span></a></li>
  </ul></div>
<div class="tabs">
  <ul>
    <li><a href="files.html"><span>File&nbsp;List</span></a></li>
    <li><a href="globals.html"><span>File&nbsp;Members</span></a></li>
  </ul></div>
<h1>AdpBP.h File Reference</h1>Offers the two versions of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). <a href="#_details">More...</a>
<p>
<code>#include &quot;Array/ArrayOp.h&quot;</code><br>
<code>#include &quot;<a class="el" href="_model_interface_8h-source.html">ReClaM/ModelInterface.h</a>&quot;</code><br>
<code>#include &lt;limits&gt;</code><br>

<p>
<a href="_adp_b_p_8h-source.html">Go to the source code of this file.</a><table border="0" cellpadding="0" cellspacing="0">
<tr><td></td></tr>
<tr><td colspan="2"><br><h2>Classes</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">class &nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_adp_b_p90a.html">AdpBP90a</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Offers the gradient-based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation).  <a href="class_adp_b_p90a.html#_details">More...</a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">class &nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_adp_b_p90b.html">AdpBP90b</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent.  <a href="class_adp_b_p90b.html#_details">More...</a><br></td></tr>
<tr><td colspan="2"><br><h2>Variables</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_adp_b_p90b.html">AdpBP90b</a> &amp;&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#faabd99e32d62d06313dac12ae518da6">in</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm.  <a href="#faabd99e32d62d06313dac12ae518da6"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_adp_b_p90b.html">AdpBP90b</a> &amp;&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#f8434108d670ecbc17d19b1540344c25">out</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm.  <a href="#f8434108d670ecbc17d19b1540344c25"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_adp_b_p90b.html">AdpBP90b</a>&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#98d9ad7e268c9990df6f886e49933903">u</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm.  <a href="#98d9ad7e268c9990df6f886e49933903"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_adp_b_p90b.html">AdpBP90b</a>&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#d3f81ed4019eaa3d16f86b235697aa0f">d</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm.  <a href="#d3f81ed4019eaa3d16f86b235697aa0f"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_adp_b_p90b.html">AdpBP90b</a>&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#ba5c1448179c5407d1e880bf71a7c67a">alpha</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm.  <a href="#ba5c1448179c5407d1e880bf71a7c67a"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">double&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#f217f961a1db7002bf2ce7a0961a4b6d">eta0</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">The initial value for the adaptive learning rate <img class="formulaInl" alt="$\eta$" src="form_86.png"> for all weights of the network.  <a href="#f217f961a1db7002bf2ce7a0961a4b6d"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">double&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#99fd879fc49b99894dca14a118bc4a53">oldE</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">The old error value.  <a href="#99fd879fc49b99894dca14a118bc4a53"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">Array&lt; double &gt;&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#bc7033c7dad621be8e5357872a751704">deltaw</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">The final update values (including the individual learning rates) for all weights.  <a href="#bc7033c7dad621be8e5357872a751704"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">Array&lt; double &gt;&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#e9ef92f25cf31ee79e03f1a9d1108609">v</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">The update values (without the individual learning rates) for all weights.  <a href="#e9ef92f25cf31ee79e03f1a9d1108609"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">Array&lt; double &gt;&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#ace70724b5b862d5e0f72cbc51374696">dedwOld</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">The last error gradient.  <a href="#ace70724b5b862d5e0f72cbc51374696"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">Array&lt; double &gt;&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="_adp_b_p_8h.html#a508d8056ca0be9e22ab8d72d1ae3122">eta</a></td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">The adaptive learning rate <img class="formulaInl" alt="$\eta$" src="form_86.png"> (for each weight individually).  <a href="#a508d8056ca0be9e22ab8d72d1ae3122"></a><br></td></tr>
</table>
<hr><a name="_details"></a><h2>Detailed Description</h2>
Offers the two versions of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). 
<p>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
<dl compact><dt><b>Copyright (c) 1999-2000:</b></dt><dd>Institut f&uuml;r Neuroinformatik<br>
 Ruhr-Universit&auml;t Bochum<br>
 D-44780 Bochum, Germany<br>
 Phone: +49-234-32-25558<br>
 Fax: +49-234-32-14209<br>
 eMail: <a href="mailto:Shark-admin@neuroinformatik.ruhr-uni-bochum.de">Shark-admin@neuroinformatik.ruhr-uni-bochum.de</a><br>
 www: <a href="http://www.neuroinformatik.ruhr-uni-bochum.de">http://www.neuroinformatik.ruhr-uni-bochum.de</a><br>
</dd></dl>
<dl compact><dt><b>Project:</b></dt><dd>ReClaM</dd></dl>
<dl compact><dt><b>Language and Compiler:</b></dt><dd>C++</dd></dl>
<dl compact><dt><b>File and Revision:</b></dt><dd><dl compact><dt><b>RCSfile</b></dt><dd><a class="el" href="_adp_b_p_8h.html">AdpBP.h</a>,v </dd></dl>
<br>
 <dl compact><dt><b>Revision</b></dt><dd>2.1 </dd></dl>
<br>
 <dl compact><dt><b>Date</b></dt><dd>2005/08/03 12:02:33 </dd></dl>
</dd></dl>
<dl compact><dt><b>Changes:</b></dt><dd><dl compact><dt><b>Log</b></dt><dd><a class="el" href="_adp_b_p_8h.html">AdpBP.h</a>,v </dd></dl>
Revision 2.1 2005/08/03 12:02:33 christian_igel changed limits/values etc.</dd></dl>
Revision 2.0 2003/11/28 16:23:21 shark-admin Revision tag reset to revision tag 2.x<p>
Revision 1.1.1.1 2003/11/24 13:37:06 shark-admin INI Administration<p>
Revision 1.4 2002/05/16 13:24:13 rudi doxygen commands added/modified.<p>
Revision 1.3 2002/02/06 14:30:12 rudi Doxygen comments added.<p>
This file is part of ReClaM. This library is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2, or (at your option) any later version.<p>
This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.<p>
You should have received a copy of the GNU General Public License along with this library; if not, write to the Free Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA. 
<p>
Definition in file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.<hr><h2>Variable Documentation</h2>
<a class="anchor" name="ba5c1448179c5407d1e880bf71a7c67a"></a><!-- doxytag: member="AdpBP.h::alpha" ref="ba5c1448179c5407d1e880bf71a7c67a" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="class_adp_b_p90b.html">AdpBP90b</a> <a class="el" href="_adp_b_p_8h.html#ba5c1448179c5407d1e880bf71a7c67a">alpha</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm. 
<p>
The error <img class="formulaInl" alt="$E^{(t)}$" src="form_83.png"> and its derivatives with respect to the model parameters for the current iteration <img class="formulaInl" alt="$t$" src="form_82.png"> are calculated and the values of the weights <img class="formulaInl" alt="$w_i,\ i = 1, \dots, n$" src="form_84.png"> and the individual learning rates <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> are adapted.<p>
<dl compact><dt><b>Parameters:</b></dt><dd>
  <table border="0" cellspacing="2" cellpadding="0">
    <tr><td valign="top"></td><td valign="top"><em>in</em>&nbsp;</td><td>The input patterns used for the training of the network. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>out</em>&nbsp;</td><td>The target values for the input patterns. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>u</em>&nbsp;</td><td>The increase factor for the adaptive learning rate. The default value is "1.2". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>d</em>&nbsp;</td><td>The decrease factor for the adaptive learning rate. The default value is "0.7". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>alpha</em>&nbsp;</td><td>The momentum factor <img class="formulaInl" alt="$\alpha$" src="form_85.png"> that controls the effect of the momentum term, by default set to "0.5". </td></tr>
  </table>
</dl>
<dl compact><dt><b>Returns:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
<dl compact><dt><b>Changes</b></dt><dd>2002-01-18, ra: <br>
 Removed some forgotten "couts" used for monitoring.</dd></dl>
<dl compact><dt><b>Status</b></dt><dd>stable </dd></dl>

<p>
Referenced by <a class="el" href="_adp_b_p_8h-source.html#l00211">AdpBP90a::adpBP()</a>, <a class="el" href="_radius_margin_8h-source.html#l00055">RadiusMargin::error()</a>, <a class="el" href="_radius_margin_8h-source.html#l00116">RadiusMargin::errorDerivative()</a>, <a class="el" href="_early_stopping_8cpp-source.html#l00314">EarlyStopping::GL()</a>, <a class="el" href="svm_8cpp-source.html#l00061">SVM::model()</a>, <a class="el" href="svm_8cpp-source.html#l00430">SVM_Optimizer::optimize()</a>, <a class="el" href="_early_stopping_8cpp-source.html#l00376">EarlyStopping::PQ()</a>, and <a class="el" href="_early_stopping_8cpp-source.html#l00344">EarlyStopping::TP()</a>.
</div>
</div><p>
<a class="anchor" name="d3f81ed4019eaa3d16f86b235697aa0f"></a><!-- doxytag: member="AdpBP.h::d" ref="d3f81ed4019eaa3d16f86b235697aa0f" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="class_adp_b_p90b.html">AdpBP90b</a> <a class="el" href="_adp_b_p_8h.html#d3f81ed4019eaa3d16f86b235697aa0f">d</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm. 
<p>
The error <img class="formulaInl" alt="$E^{(t)}$" src="form_83.png"> and its derivatives with respect to the model parameters for the current iteration <img class="formulaInl" alt="$t$" src="form_82.png"> are calculated and the values of the weights <img class="formulaInl" alt="$w_i,\ i = 1, \dots, n$" src="form_84.png"> and the individual learning rates <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> are adapted.<p>
<dl compact><dt><b>Parameters:</b></dt><dd>
  <table border="0" cellspacing="2" cellpadding="0">
    <tr><td valign="top"></td><td valign="top"><em>in</em>&nbsp;</td><td>The input patterns used for the training of the network. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>out</em>&nbsp;</td><td>The target values for the input patterns. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>u</em>&nbsp;</td><td>The increase factor for the adaptive learning rate. The default value is "1.2". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>d</em>&nbsp;</td><td>The decrease factor for the adaptive learning rate. The default value is "0.7". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>alpha</em>&nbsp;</td><td>The momentum factor <img class="formulaInl" alt="$\alpha$" src="form_85.png"> that controls the effect of the momentum term, by default set to "0.5". </td></tr>
  </table>
</dl>
<dl compact><dt><b>Returns:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
<dl compact><dt><b>Changes</b></dt><dd>2002-01-18, ra: <br>
 Removed some forgotten "couts" used for monitoring.</dd></dl>
<dl compact><dt><b>Status</b></dt><dd>stable </dd></dl>

<p>
Referenced by <a class="el" href="_adp_b_p_8h-source.html#l00211">AdpBP90a::adpBP()</a>, <a class="el" href="_concatenated_model_8h-source.html#l00086">ConcatenatedModel::AppendModel()</a>, <a class="el" href="_b_f_g_s_8cpp-source.html#l00145">BFGS::bfgs()</a>, <a class="el" href="_gauss_kernel_8h-source.html#l00199">GeneralGaussKernel::computeGamma()</a>, <a class="el" href="_k_t_a_8h-source.html#l00307">negativeBKTA::errorDerivative()</a>, <a class="el" href="_k_t_a_8h-source.html#l00095">negativeKTA::errorDerivative()</a>, <a class="el" href="_gauss_kernel_8h-source.html#l00113">GeneralGaussKernel::eval()</a>, <a class="el" href="_gauss_kernel_8h-source.html#l00137">GeneralGaussKernel::evalDerivative()</a>, <a class="el" href="opt_grid_8h-source.html#l00280">optGridNested::init()</a>, <a class="el" href="opt_grid_8h-source.html#l00145">optGrid::init()</a>, <a class="el" href="svm_8cpp-source.html#l00199">SVM::LoadSVMModel()</a>, <a class="el" href="_sigmoid_model_8h-source.html#l00111">SigmoidModel::modelDerivative()</a>, <a class="el" href="opt_sigmoid_fit_8h-source.html#l00109">optSigmoidFit::optimize()</a>, <a class="el" href="opt_grid_8h-source.html#l00309">optGridNested::optimize()</a>, <a class="el" href="_cross_validation_8h-source.html#l00087">Partitioning::Partitioning()</a>, and <a class="el" href="svm_8cpp-source.html#l00269">SVM::SaveSVMModel()</a>.
</div>
</div><p>
<a class="anchor" name="ace70724b5b862d5e0f72cbc51374696"></a><!-- doxytag: member="AdpBP.h::dedwOld" ref="ace70724b5b862d5e0f72cbc51374696" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Array&lt;double&gt; <a class="el" href="_adp_b_p_8h.html#ace70724b5b862d5e0f72cbc51374696">dedwOld</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
The last error gradient. 
<p>

<p>
Definition at line <a class="el" href="_adp_b_p_8h-source.html#l00496">496</a> of file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.
<p>
Referenced by <a class="el" href="_adp_b_p_8h-source.html#l00370">AdpBP90b::initAdpBP()</a>.
</div>
</div><p>
<a class="anchor" name="bc7033c7dad621be8e5357872a751704"></a><!-- doxytag: member="AdpBP.h::deltaw" ref="bc7033c7dad621be8e5357872a751704" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Array&lt;double&gt; <a class="el" href="_adp_b_p_8h.html#bc7033c7dad621be8e5357872a751704">deltaw</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
The final update values (including the individual learning rates) for all weights. 
<p>

<p>
Definition at line <a class="el" href="_adp_b_p_8h-source.html#l00489">489</a> of file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.
<p>
Referenced by <a class="el" href="_adp_b_p_8h-source.html#l00370">AdpBP90b::initAdpBP()</a>.
</div>
</div><p>
<a class="anchor" name="a508d8056ca0be9e22ab8d72d1ae3122"></a><!-- doxytag: member="AdpBP.h::eta" ref="a508d8056ca0be9e22ab8d72d1ae3122" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Array&lt;double&gt; <a class="el" href="_adp_b_p_8h.html#a508d8056ca0be9e22ab8d72d1ae3122">eta</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
The adaptive learning rate <img class="formulaInl" alt="$\eta$" src="form_86.png"> (for each weight individually). 
<p>

<p>
Definition at line <a class="el" href="_adp_b_p_8h-source.html#l00500">500</a> of file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.
<p>
Referenced by <a class="el" href="_adp_b_p_8h-source.html#l00370">AdpBP90b::initAdpBP()</a>.
</div>
</div><p>
<a class="anchor" name="f217f961a1db7002bf2ce7a0961a4b6d"></a><!-- doxytag: member="AdpBP.h::eta0" ref="f217f961a1db7002bf2ce7a0961a4b6d" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="_adp_b_p_8h.html#f217f961a1db7002bf2ce7a0961a4b6d">eta0</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
The initial value for the adaptive learning rate <img class="formulaInl" alt="$\eta$" src="form_86.png"> for all weights of the network. 
<p>

<p>
Definition at line <a class="el" href="_adp_b_p_8h-source.html#l00482">482</a> of file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.
<p>
Referenced by <a class="el" href="_adp_b_p_8h-source.html#l00370">AdpBP90b::initAdpBP()</a>.
</div>
</div><p>
<a class="anchor" name="faabd99e32d62d06313dac12ae518da6"></a><!-- doxytag: member="AdpBP.h::in" ref="faabd99e32d62d06313dac12ae518da6" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="class_adp_b_p90b.html">AdpBP90b</a> &amp; <a class="el" href="_adp_b_p_8h.html#faabd99e32d62d06313dac12ae518da6">in</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm. 
<p>
The error <img class="formulaInl" alt="$E^{(t)}$" src="form_83.png"> and its derivatives with respect to the model parameters for the current iteration <img class="formulaInl" alt="$t$" src="form_82.png"> are calculated and the values of the weights <img class="formulaInl" alt="$w_i,\ i = 1, \dots, n$" src="form_84.png"> and the individual learning rates <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> are adapted.<p>
<dl compact><dt><b>Parameters:</b></dt><dd>
  <table border="0" cellspacing="2" cellpadding="0">
    <tr><td valign="top"></td><td valign="top"><em>in</em>&nbsp;</td><td>The input patterns used for the training of the network. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>out</em>&nbsp;</td><td>The target values for the input patterns. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>u</em>&nbsp;</td><td>The increase factor for the adaptive learning rate. The default value is "1.2". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>d</em>&nbsp;</td><td>The decrease factor for the adaptive learning rate. The default value is "0.7". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>alpha</em>&nbsp;</td><td>The momentum factor <img class="formulaInl" alt="$\alpha$" src="form_85.png"> that controls the effect of the momentum term, by default set to "0.5". </td></tr>
  </table>
</dl>
<dl compact><dt><b>Returns:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
<dl compact><dt><b>Changes</b></dt><dd>2002-01-18, ra: <br>
 Removed some forgotten "couts" used for monitoring.</dd></dl>
<dl compact><dt><b>Status</b></dt><dd>stable </dd></dl>
<dl compact><dt><b>Examples: </b></dt><dd>
<a class="el" href="simple_f_f_net_8cpp-example.html#a12">simpleFFNet.cpp</a>.</dl>
<p>
Referenced by <a class="el" href="_f_f_net_8h-source.html#l00472">FFNet::activate()</a>, <a class="el" href="_i_o_tools_8h-source.html#l00151">loadOrganizedData()</a>, and <a class="el" href="_component_wise_model_8h-source.html#l00089">ComponentWiseModel::model()</a>.
</div>
</div><p>
<a class="anchor" name="99fd879fc49b99894dca14a118bc4a53"></a><!-- doxytag: member="AdpBP.h::oldE" ref="99fd879fc49b99894dca14a118bc4a53" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="_adp_b_p_8h.html#99fd879fc49b99894dca14a118bc4a53">oldE</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
The old error value. 
<p>

<p>
Definition at line <a class="el" href="_adp_b_p_8h-source.html#l00485">485</a> of file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.
<p>
Referenced by <a class="el" href="_adp_b_p_8h-source.html#l00370">AdpBP90b::initAdpBP()</a>.
</div>
</div><p>
<a class="anchor" name="f8434108d670ecbc17d19b1540344c25"></a><!-- doxytag: member="AdpBP.h::out" ref="f8434108d670ecbc17d19b1540344c25" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="class_adp_b_p90b.html">AdpBP90b</a> &amp; <a class="el" href="_adp_b_p_8h.html#f8434108d670ecbc17d19b1540344c25">out</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm. 
<p>
The error <img class="formulaInl" alt="$E^{(t)}$" src="form_83.png"> and its derivatives with respect to the model parameters for the current iteration <img class="formulaInl" alt="$t$" src="form_82.png"> are calculated and the values of the weights <img class="formulaInl" alt="$w_i,\ i = 1, \dots, n$" src="form_84.png"> and the individual learning rates <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> are adapted.<p>
<dl compact><dt><b>Parameters:</b></dt><dd>
  <table border="0" cellspacing="2" cellpadding="0">
    <tr><td valign="top"></td><td valign="top"><em>in</em>&nbsp;</td><td>The input patterns used for the training of the network. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>out</em>&nbsp;</td><td>The target values for the input patterns. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>u</em>&nbsp;</td><td>The increase factor for the adaptive learning rate. The default value is "1.2". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>d</em>&nbsp;</td><td>The decrease factor for the adaptive learning rate. The default value is "0.7". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>alpha</em>&nbsp;</td><td>The momentum factor <img class="formulaInl" alt="$\alpha$" src="form_85.png"> that controls the effect of the momentum term, by default set to "0.5". </td></tr>
  </table>
</dl>
<dl compact><dt><b>Returns:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
<dl compact><dt><b>Changes</b></dt><dd>2002-01-18, ra: <br>
 Removed some forgotten "couts" used for monitoring.</dd></dl>
<dl compact><dt><b>Status</b></dt><dd>stable </dd></dl>
<dl compact><dt><b>Examples: </b></dt><dd>
<a class="el" href="simple_f_f_net_8cpp-example.html#a13">simpleFFNet.cpp</a>.</dl>
<p>
Referenced by <a class="el" href="_squared_error_8h-source.html#l00179">SquaredError::derror()</a>, <a class="el" href="_squared_error_8h-source.html#l00122">SquaredError::error()</a>, <a class="el" href="_error_measures_8h-source.html#l00139">ErrorMeasures::errorPercentage()</a>, <a class="el" href="_i_o_tools_8h-source.html#l00151">loadOrganizedData()</a>, and <a class="el" href="_component_wise_model_8h-source.html#l00089">ComponentWiseModel::model()</a>.
</div>
</div><p>
<a class="anchor" name="98d9ad7e268c9990df6f886e49933903"></a><!-- doxytag: member="AdpBP.h::u" ref="98d9ad7e268c9990df6f886e49933903" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="class_adp_b_p90b.html">AdpBP90b</a> <a class="el" href="_adp_b_p_8h.html#98d9ad7e268c9990df6f886e49933903">u</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Offers the second version of the gradient descent based optimization algorithm with individual adaptive learning rates by Silva and Almeida (Adaptive BackPropagation). This optimization algorithm introduced by Silva and Almeida adds individual adaptive learning rates and weight-backtracking to standard steepest descent. Performs a run of the optimization algorithm. 
<p>
The error <img class="formulaInl" alt="$E^{(t)}$" src="form_83.png"> and its derivatives with respect to the model parameters for the current iteration <img class="formulaInl" alt="$t$" src="form_82.png"> are calculated and the values of the weights <img class="formulaInl" alt="$w_i,\ i = 1, \dots, n$" src="form_84.png"> and the individual learning rates <img class="formulaInl" alt="$\eta_i$" src="form_78.png"> are adapted.<p>
<dl compact><dt><b>Parameters:</b></dt><dd>
  <table border="0" cellspacing="2" cellpadding="0">
    <tr><td valign="top"></td><td valign="top"><em>in</em>&nbsp;</td><td>The input patterns used for the training of the network. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>out</em>&nbsp;</td><td>The target values for the input patterns. </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>u</em>&nbsp;</td><td>The increase factor for the adaptive learning rate. The default value is "1.2". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>d</em>&nbsp;</td><td>The decrease factor for the adaptive learning rate. The default value is "0.7". </td></tr>
    <tr><td valign="top"></td><td valign="top"><em>alpha</em>&nbsp;</td><td>The momentum factor <img class="formulaInl" alt="$\alpha$" src="form_85.png"> that controls the effect of the momentum term, by default set to "0.5". </td></tr>
  </table>
</dl>
<dl compact><dt><b>Returns:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Author:</b></dt><dd>C. Igel </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>1999</dd></dl>
<dl compact><dt><b>Changes</b></dt><dd>2002-01-18, ra: <br>
 Removed some forgotten "couts" used for monitoring.</dd></dl>
<dl compact><dt><b>Status</b></dt><dd>stable </dd></dl>
<dl compact><dt><b>Examples: </b></dt><dd>
<a class="el" href="simple_f_f_net_8cpp-example.html#a10">simpleFFNet.cpp</a>.</dl>
<p>
Referenced by <a class="el" href="_adp_b_p_8h-source.html#l00211">AdpBP90a::adpBP()</a>.
</div>
</div><p>
<a class="anchor" name="e9ef92f25cf31ee79e03f1a9d1108609"></a><!-- doxytag: member="AdpBP.h::v" ref="e9ef92f25cf31ee79e03f1a9d1108609" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Array&lt;double&gt; <a class="el" href="_adp_b_p_8h.html#e9ef92f25cf31ee79e03f1a9d1108609">v</a>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
The update values (without the individual learning rates) for all weights. 
<p>
<dl compact><dt><b>Examples: </b></dt><dd>
<a class="el" href="simple_f_f_net_8cpp-example.html#a11">simpleFFNet.cpp</a>, and <a class="el" href="simple_r_b_f_net_8cpp-example.html#a20">simpleRBFNet.cpp</a>.</dl>
<p>
Definition at line <a class="el" href="_adp_b_p_8h-source.html#l00493">493</a> of file <a class="el" href="_adp_b_p_8h-source.html">AdpBP.h</a>.
<p>
Referenced by <a class="el" href="_gauss_kernel_8h-source.html#l00199">GeneralGaussKernel::computeGamma()</a>, <a class="el" href="_r_b_f_net_8h-source.html#l00518">RBFNet::getNHidden()</a>, <a class="el" href="_r_b_f_net_8h-source.html#l00497">RBFNet::getVariance()</a>, <a class="el" href="_adp_b_p_8h-source.html#l00370">AdpBP90b::initAdpBP()</a>, <a class="el" href="svm_8cpp-source.html#l00109">SVM::modelDerivative()</a>, <a class="el" href="_f_f_net_8cpp-source.html#l01204">operator&gt;&gt;()</a>, <a class="el" href="_r_b_f_net_8h-source.html#l00178">RBFNet::RBFNet()</a>, <a class="el" href="_r_b_f_net_8h-source.html#l00629">RBFNet::setVariance()</a>, <a class="el" href="_c___solver_8cpp-source.html#l00588">C_Solver::Shrink()</a>, and <a class="el" href="_c___solver_8cpp-source.html#l00692">C_Solver::Unshrink()</a>.
</div>
</div><p>
<hr size="1"><address style="align: right;"><small>Generated on Wed Dec 16 13:07:16 2009 for ReClaM by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.4.7 </small></address>
</body>
</html>
