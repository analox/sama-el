<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
<title>ReClaM: simpleRNNet.cpp</title>
<link href="ReClaM.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.4.7 -->
<div class="tabs">
  <ul>
    <li><a href="index.html"><span>Main&nbsp;Page</span></a></li>
    <li><a href="namespaces.html"><span>Namespaces</span></a></li>
    <li><a href="annotated.html"><span>Classes</span></a></li>
    <li><a href="files.html"><span>Files</span></a></li>
    <li><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
    <li><a href="examples.html"><span>Examples</span></a></li>
  </ul></div>
<h1>simpleRNNet.cpp</h1>A recurrent neural network regression model that learns with Back Propagation Through Time and assumes the MSE error functional. This class defines a recurrent neural network regression model. The input and output arrays should, as usual, be 2-dimensional and have the dimensionality (batch-length,number-of-neurons). The only difference is that here, the batch corresponds to a time series instead of independent data points. The gradient is calculated via BackPropTroughTime (BPTT). This model implies the MSE as the error functional.<p>
The class can handle arbitrary network architectures. Feed-forward connections (with time delay `zero') as well as connections of any time delay can be realized --- see setStructure() for details of how to define the structure.<p>
Things to note!:<p>
(1) All neurons are sigmoidal! So please transform inputs and outputs to take this into account. The reason is that internally, no differences between input, hidden, or output neurons is made.<p>
(2) The initialization of the state history is an important issue for dynamic systems like an RNN. Please read the documentation of setWarmUpLentgh().<p>
(3) Online learning can, in principle, be realized by having a batch-length of zero (the input/output arrays still have to be 2-dimensional, i.e., of dimension (1,number-of-neurons)) and by setting the WarmUpLength to zero. Note though that this is quite inefficient compared to batch-learning (because the BPTT does not save you any calculation time) and that cross-talk will make learning _very_ difficult. Maybe, even when online learning, you should still use a reasonable batch size (say 100), feed these batches sequentially, and set the WarmUpLength to zero.<p>
Please also refer to the<p>
(Internals: the only core functions are `processTimeSeries' and `calcGradBPTT' in the cpp-file. Almost all the rest it `utility' stuff.)<p>
<dl compact><dt><b>Author:</b></dt><dd>M. Toussaint </dd></dl>
<dl compact><dt><b>Date:</b></dt><dd>2000</dd></dl>
<dl compact><dt><b>Changes:</b></dt><dd>none</dd></dl>
<dl compact><dt><b>Status:</b></dt><dd>stable</dd></dl>
<div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="comment">//===========================================================================</span>
<a name="l00051"></a>00051 <span class="comment"></span><span class="comment">//===========================================================================</span>
<a name="l00052"></a>00052 
<a name="l00053"></a>00053 
<a name="l00054"></a>00054 <span class="preprocessor">#include "<a class="code" href="_m_s_e_r_n_net_8h.html">ReClaM/MSERNNet.h</a>"</span>
<a name="l00055"></a>00055 <span class="preprocessor">#include "<a class="code" href="_rprop_8h.html">ReClaM/Rprop.h</a>"</span>
<a name="l00056"></a>00056 <span class="preprocessor">#include "Array/Array.h"</span>
<a name="l00057"></a>00057 
<a name="l00058"></a>00058 <span class="keyword">using namespace </span>std;
<a name="l00059"></a>00059 
<a name="l00060"></a>00060 <span class="comment">// Define own network class to combine a recurrent</span>
<a name="l00061"></a>00061 <span class="comment">// neural network with a mean squared error measure</span>
<a name="l00062"></a>00062 <span class="comment">// and the RProp algorithm for optimization:</span>
<a name="l00063"></a>00063 <span class="comment">//</span>
<a name="l00064"></a>00064 <span class="keyword">class </span>MyNet : <span class="keyword">public</span> <a name="_a21"></a><a class="code" href="class_i_rprop_plus.html">IRpropPlus</a>, <span class="keyword">public</span> <a name="_a22"></a><a class="code" href="class_m_s_e_r_n_net.html">MSERNNet</a> 
<a name="l00065"></a>00065 {
<a name="l00066"></a>00066     <span class="keyword">public</span>:
<a name="l00067"></a>00067 
<a name="l00068"></a>00068         <span class="comment">// Standard network constructor:</span>
<a name="l00069"></a>00069         MyNet( ) : <a name="a23"></a><a class="code" href="class_m_s_e_r_n_net.html#3f78d3a1aadafd362e13f6f84c5c9da7">MSERNNet</a>( ) { };
<a name="l00070"></a>00070 
<a name="l00071"></a>00071         <span class="comment">// Constructor for a network based on a given connection</span>
<a name="l00072"></a>00072         <span class="comment">// matrix:</span>
<a name="l00073"></a>00073         MyNet( Array&lt; int &gt; con ) : <a class="code" href="class_m_s_e_r_n_net.html#3f78d3a1aadafd362e13f6f84c5c9da7">MSERNNet</a>( con ) { };
<a name="l00074"></a>00074     
<a name="l00075"></a>00075         <span class="comment">// This method will return the derivatives of the error</span>
<a name="l00076"></a>00076         <span class="comment">// of the network.</span>
<a name="l00077"></a>00077         Array&lt; double&gt; getDedw( ){ <span class="keywordflow">return</span> dedw; }
<a name="l00078"></a>00078 
<a name="l00079"></a>00079   
<a name="l00080"></a>00080     <span class="keyword">private</span>:
<a name="l00081"></a>00081 
<a name="l00082"></a>00082         <span class="comment">// Define own activation function for the</span>
<a name="l00083"></a>00083         <span class="comment">// hidden neurons, corresponding to the</span>
<a name="l00084"></a>00084         <span class="comment">// prediction task (see below):</span>
<a name="l00085"></a>00085         <span class="keywordtype">double</span> <a name="a24"></a><a class="code" href="class_m_s_e_r_n_net.html#0be5954de6094315575d7aa5e9298dcc">g</a>( <span class="keywordtype">double</span> a )  { <span class="keywordflow">return</span> tanh( a ); }
<a name="l00086"></a>00086 
<a name="l00087"></a>00087         <span class="comment">// A way to calculate the derivative of the</span>
<a name="l00088"></a>00088         <span class="comment">// activation function, when given the result</span>
<a name="l00089"></a>00089         <span class="comment">// of the activation:</span>
<a name="l00090"></a>00090         <span class="keywordtype">double</span> <a name="a25"></a><a class="code" href="class_m_s_e_r_n_net.html#94474549ffa45de0a108c1de61883380">dg</a>( <span class="keywordtype">double</span> ga ) { <span class="keywordflow">return</span> 1 - ga * ga; }
<a name="l00091"></a>00091 
<a name="l00092"></a>00092 };
<a name="l00093"></a>00093 
<a name="l00094"></a>00094 
<a name="l00095"></a>00095 <span class="keywordtype">int</span> main()
<a name="l00096"></a>00096 {
<a name="l00097"></a>00097     <span class="comment">// Just counter variables:</span>
<a name="l00098"></a>00098     <span class="keywordtype">unsigned</span> i, j, k; 
<a name="l00099"></a>00099 
<a name="l00100"></a>00100     <span class="keywordtype">unsigned</span> numberOfHiddenNeurons( 5 );
<a name="l00101"></a>00101 
<a name="l00102"></a>00102     <span class="comment">// We use a network here with one input, one</span>
<a name="l00103"></a>00103     <span class="comment">// output and five hidden neurons:</span>
<a name="l00104"></a>00104     <span class="keywordtype">unsigned</span> numberOfNeurons = numberOfHiddenNeurons + 1 + 1;
<a name="l00105"></a>00105 
<a name="l00106"></a>00106     <span class="comment">// We will use the network for a mini time series,</span>
<a name="l00107"></a>00107     <span class="comment">// where we will predict the value for one step in</span>
<a name="l00108"></a>00108     <span class="comment">// the future:</span>
<a name="l00109"></a>00109     <span class="keywordtype">unsigned</span> numberOfTimeSteps = 2;
<a name="l00110"></a>00110 
<a name="l00111"></a>00111     <span class="comment">// We will use 100 input patterns...</span>
<a name="l00112"></a>00112     <span class="keywordtype">unsigned</span> numberOfDataPoints = 100;
<a name="l00113"></a>00113 
<a name="l00114"></a>00114     <span class="keywordtype">unsigned</span> numberOfLearningCycles = 100;
<a name="l00115"></a>00115 
<a name="l00116"></a>00116     <span class="comment">// ..., where 90 will be used for training</span>
<a name="l00117"></a>00117     <span class="comment">// and the rest for initialization of the</span>
<a name="l00118"></a>00118     <span class="comment">// network:</span>
<a name="l00119"></a>00119     <span class="keywordtype">unsigned</span> warmUpLength = 10;
<a name="l00120"></a>00120 
<a name="l00121"></a>00121 
<a name="l00122"></a>00122     <span class="comment">// Create connection matrix for two time steps and</span>
<a name="l00123"></a>00123     <span class="comment">// 7 neurons:</span>
<a name="l00124"></a>00124     Array&lt; int &gt; con( numberOfTimeSteps, numberOfNeurons, numberOfNeurons );
<a name="l00125"></a>00125 
<a name="l00126"></a>00126     <span class="comment">// For the initial time step establish connections</span>
<a name="l00127"></a>00127     <span class="comment">// from the input neuron to each hidden neuron and</span>
<a name="l00128"></a>00128     <span class="comment">// from each hidden neuron to the output neuron:</span>
<a name="l00129"></a>00129     <span class="keywordflow">for</span> ( i = 1; i &lt; con.dim( 1 ); i++ )
<a name="l00130"></a>00130     {
<a name="l00131"></a>00131         <span class="keywordflow">for</span> ( j = 0; j &lt;= i; j++ )
<a name="l00132"></a>00132         {
<a name="l00133"></a>00133             con( 0,i,j ) = 1;
<a name="l00134"></a>00134         }
<a name="l00135"></a>00135     }
<a name="l00136"></a>00136 
<a name="l00137"></a>00137     <span class="comment">// For the second time step establish connections</span>
<a name="l00138"></a>00138     <span class="comment">// between all neurons:</span>
<a name="l00139"></a>00139     <span class="keywordflow">for</span> ( k = 1; k &lt; con.dim( 0 ); k++ )
<a name="l00140"></a>00140     {
<a name="l00141"></a>00141         <span class="keywordflow">for</span> ( i = 0; i &lt; con.dim( 1 ); i++ )
<a name="l00142"></a>00142         {
<a name="l00143"></a>00143             <span class="keywordflow">for</span> ( j = 0; j &lt; con.dim( 2 ); j++ )
<a name="l00144"></a>00144             {
<a name="l00145"></a>00145                 con( k, i, j ) = 1;
<a name="l00146"></a>00146             }
<a name="l00147"></a>00147         } 
<a name="l00148"></a>00148     } 
<a name="l00149"></a>00149 
<a name="l00150"></a>00150     <span class="comment">// Construct new network object on the base of the</span>
<a name="l00151"></a>00151     <span class="comment">// connection matrix:</span>
<a name="l00152"></a>00152     MyNet net( con );
<a name="l00153"></a>00153 
<a name="l00154"></a>00154     <span class="comment">// Initialize the weights of the neuron connections</span>
<a name="l00155"></a>00155     <span class="comment">// by uniformally distributed random numbers between</span>
<a name="l00156"></a>00156     <span class="comment">// "-0.1" and "0.1":</span>
<a name="l00157"></a>00157     net.initWeights( -0.1, 0.1 );
<a name="l00158"></a>00158 
<a name="l00159"></a>00159     <span class="comment">// Use the first ten input patterns for initializing</span>
<a name="l00160"></a>00160     <span class="comment">// the internal state of the network:</span>
<a name="l00161"></a>00161     net.includeWarmUp( warmUpLength );
<a name="l00162"></a>00162 
<a name="l00163"></a>00163     <span class="comment">// Create the data set.  </span>
<a name="l00164"></a>00164     Array&lt; double &gt; input(  numberOfDataPoints, 1 ), 
<a name="l00165"></a>00165                     target( numberOfDataPoints, 1 );
<a name="l00166"></a>00166 
<a name="l00167"></a>00167     <span class="comment">// The task is to predict the next amplitude of an sin-like </span>
<a name="l00168"></a>00168     <span class="comment">// oszillation:</span>
<a name="l00169"></a>00169     <span class="keywordflow">for</span> ( i = 0; i &lt; input.dim( 0 ); i++ )
<a name="l00170"></a>00170     {
<a name="l00171"></a>00171         input( i, 0 )  = sin( 0.1 * i );
<a name="l00172"></a>00172         target( i, 0 ) = sin( 0.1 * ( i + 1 ) );
<a name="l00173"></a>00173     }
<a name="l00174"></a>00174 
<a name="l00175"></a>00175 
<a name="l00176"></a>00176     <span class="comment">// Initialize the RProp optimization algorithm:</span>
<a name="l00177"></a>00177     net.initRprop( 0.01 );
<a name="l00178"></a>00178 
<a name="l00179"></a>00179    
<a name="l00180"></a>00180     cout &lt;&lt; <span class="stringliteral">"Train network:\n"</span> &lt;&lt; endl 
<a name="l00181"></a>00181          &lt;&lt; <span class="stringliteral">"No. of cycles\tIteration Error:"</span> &lt;&lt; endl
<a name="l00182"></a>00182          &lt;&lt; <span class="stringliteral">"--------------------------------"</span> &lt;&lt; endl;
<a name="l00183"></a>00183 
<a name="l00184"></a>00184     <span class="comment">// Train the network by using the RProp algorithm,</span>
<a name="l00185"></a>00185     <span class="comment">// monitor error each 10 training cycles:</span>
<a name="l00186"></a>00186     <span class="keywordflow">for</span> ( i = 1; i &lt;= numberOfLearningCycles; i++ )
<a name="l00187"></a>00187     {
<a name="l00188"></a>00188         net.rprop( input, target );
<a name="l00189"></a>00189         <span class="keywordflow">if</span> ( i % 10 == 0 ) 
<a name="l00190"></a>00190         {
<a name="l00191"></a>00191             cout &lt;&lt; i &lt;&lt; <span class="stringliteral">"\t\t"</span> 
<a name="l00192"></a>00192                  &lt;&lt; net.error( input, target ) &lt;&lt; endl;
<a name="l00193"></a>00193         }
<a name="l00194"></a>00194     }
<a name="l00195"></a>00195    
<a name="l00196"></a>00196     <span class="comment">// Get the prediction results of the trained network:</span>
<a name="l00197"></a>00197     Array&lt; double &gt; output( target.dim( 0 ), target.dim( 1 ) );
<a name="l00198"></a>00198     net.model( input, output );
<a name="l00199"></a>00199   
<a name="l00200"></a>00200     cout &lt;&lt; endl &lt;&lt; <span class="stringliteral">"Evaluate trained network:\n"</span> &lt;&lt; endl 
<a name="l00201"></a>00201          &lt;&lt; <span class="stringliteral">"x( t ):\t\tx( t + 1 ):\tprediction:"</span> &lt;&lt; endl
<a name="l00202"></a>00202          &lt;&lt; <span class="stringliteral">"-------------------------------------------"</span> &lt;&lt; endl; 
<a name="l00203"></a>00203 
<a name="l00204"></a>00204     <span class="comment">// Output of each data input value, its value</span>
<a name="l00205"></a>00205     <span class="comment">// in the next time step and the prediction of the</span>
<a name="l00206"></a>00206     <span class="comment">// trained network:</span>
<a name="l00207"></a>00207     <span class="keywordflow">for</span> ( i = warmUpLength; i &lt; target.dim( 0 ); i++ ) 
<a name="l00208"></a>00208     {
<a name="l00209"></a>00209         cout &lt;&lt; input( i, 0 ) &lt;&lt; <span class="stringliteral">"\t"</span> &lt;&lt; target( i, 0 ) &lt;&lt; <span class="stringliteral">"\t"</span> 
<a name="l00210"></a>00210              &lt;&lt; output( i, 0 ) &lt;&lt; endl;
<a name="l00211"></a>00211     }
<a name="l00212"></a>00212 
<a name="l00213"></a>00213     exit( EXIT_SUCCESS );
<a name="l00214"></a>00214 }
</pre></div> <hr size="1"><address style="align: right;"><small>Generated on Wed Dec 16 13:07:16 2009 for ReClaM by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.4.7 </small></address>
</body>
</html>
